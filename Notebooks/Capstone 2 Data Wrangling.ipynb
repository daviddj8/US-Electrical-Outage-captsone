{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dffc2c59-4055-43a5-b02e-28f422690e80",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f168b2-188b-476e-8a1c-79829cd9433f",
   "metadata": {},
   "source": [
    "This analysis will include data from three sources:\n",
    "1. **Outage Data**: Major US electrical outages from 2018 - 2023 as recorded by the Department of Energy (DOE)(https://www.oe.netl.doe.gov/OE417_annual_summary.aspx)\n",
    "2. **Weather Data**: Local weather at the time of the outage event as recorded by the National Oceanic and Atomospheric Administration (NOAA) (https://www.ncdc.noaa.gov/cdo-web/webservices/v2#gettingStarted)\n",
    "3. **Energy Demand Data**: Levels of energy demand at the time and location of the outage event as recorded by the Energy Information Administration (EIA) (https://www.eia.gov/opendata/documentation.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddff91b8-2a88-484a-8d6d-4699c1ee4192",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12490c3f-581d-44da-9286-6707ae4155d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules to collect data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import statistics\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5711ab73-5fed-4901-831f-e38fd471a23d",
   "metadata": {},
   "source": [
    "## Outage Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db5637c-a549-48ed-b0c1-628578dc8aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The outage data is formatted as a separate excel spreadsheet for each year.  \n",
    "# The code below imports each spreadsheet from 2018 - 2023 and combines all the separate excel sheets into one dataframe.\n",
    "# Note: xlrd needs to be installed in order to execute the code below\n",
    "\n",
    "# Create an empty list to store the dataframes for each year\n",
    "outage_dfs = []\n",
    "\n",
    "# Create a dataframe for each year by looping through years 2018 - 2023\n",
    "for year in range(2018, 2024):\n",
    "    start_row = 2\n",
    "    df = pd.read_excel(f'../Data-Wrangling/Raw-Data/Outage-Data-Raw/{year}_Annual_Summary.xls', skiprows = start_row - 1)\n",
    "    outage_dfs.append(df)\n",
    "\n",
    "# Concatenate all the dataframes stored in the dfs list to form a final combined outage dataframe\n",
    "outage_df = pd.concat(outage_dfs, ignore_index = True)\n",
    "\n",
    "# Some excel sheets had the column name \"Month\" while others had the column name \"Event Month\".\n",
    "# The code below combines this into a single column named \"Event Month\"\n",
    "outage_df['Month'].fillna(outage_df['Event Month'], inplace = True)\n",
    "outage_df.drop(columns = ['Event Month'], inplace = True)\n",
    "outage_df.rename(columns = {'Month' : 'Event Month'}, inplace = True)\n",
    "\n",
    "# The dataset includes 'unknown' values that are not registering as null values.\n",
    "# The code below converts 'unknown' values to null values\n",
    "outage_df.replace('.*Unknown.*', pd.NA, inplace = True, regex = True)\n",
    "\n",
    "# The code below converts columns to the proper data type\n",
    "\n",
    "    # 'Date Event Began' and 'Date of Restoration' should be converted to datetimes:\n",
    "outage_df['Date Event Began'] = pd.to_datetime(outage_df['Date Event Began'])\n",
    "outage_df['Date of Restoration'] = pd.to_datetime(outage_df['Date of Restoration'], format = '%m/%d/%Y', errors = 'coerce')\n",
    "\n",
    "    # 'Time Event Began' and 'Time of Restoration' can be added into their respective 'Date' columns and converted to datetimes\n",
    "    # Convert 'Time Event Began' to a timedelta, combine with 'Date Event Began', and rename the column 'Datetime Event Began'\n",
    "outage_df['Timedelta_begin'] = outage_df['Time Event Began'].apply(lambda x: pd.to_timedelta(x.strftime('%H:%M:%S')))\n",
    "outage_df['Datetime Event Began'] = outage_df['Date Event Began'] + outage_df['Timedelta_begin']\n",
    "\n",
    "    # Convert 'Time of Restoration' to a timedelta, combine with 'Date of Restoration', and rename the column 'Datetime of Restoration'\n",
    "    # First, the missing time values must be replaced with a default time\n",
    "default_time = '00:00:00'\n",
    "outage_df['Time of Restoration'].fillna(default_time, inplace=True)\n",
    "\n",
    "    # Two of the 'Time of Restoration' values are improperly formatted because of incomplete information.  The code below corrects this\n",
    "outage_df.iloc[1956, 4] = '16:41:00'\n",
    "outage_df.iloc[1956, 4]\n",
    "\n",
    "outage_df.iloc[1983, 4] = '00:00:00'\n",
    "outage_df.iloc[1983, 4]\n",
    "\n",
    "    # The code below completes the conversion of the 'Time of Restoration' column and creates a new column for 'Datetime of Restoration'\n",
    "outage_df['Time of Restoration'] = pd.to_datetime(outage_df['Time of Restoration'], format='%H:%M:%S').dt.time\n",
    "outage_df['Timedelta_restoration'] = outage_df['Time of Restoration'].apply(lambda x: pd.Timedelta(hours=x.hour, minutes=x.minute, seconds=x.second))\n",
    "outage_df['Datetime of Restoration'] = outage_df['Date of Restoration'] + outage_df['Timedelta_restoration']\n",
    "\n",
    "    # The event month, original date / time columns, and intermediate timedelta columns can be dropped and the columns can be rearranged to have the datetimes as the first columns\n",
    "outage_df.drop(columns = ['Event Month','Date Event Began','Time Event Began', 'Date of Restoration', 'Time of Restoration', 'Timedelta_begin','Timedelta_restoration'], inplace = True)\n",
    "outage_df = outage_df[['Datetime Event Began','Datetime of Restoration','Area Affected','NERC Region','Alert Criteria','Event Type','Demand Loss (MW)','Number of Customers Affected']]\n",
    "outage_df.dtypes\n",
    "\n",
    "    # 'Demand Loss' and 'Number of Customers Affected' should be converted to numeric values\n",
    "outage_df['Demand Loss (MW)'] = pd.to_numeric(outage_df['Demand Loss (MW)'], errors = 'coerce')\n",
    "outage_df['Number of Customers Affected'] = pd.to_numeric(outage_df['Number of Customers Affected'], errors = 'coerce')\n",
    "\n",
    "# Some of the rows represent events that are not true outages, these rows should be deleted from  the dataset\n",
    "    # Some rows represent events that are warnings to the public to reduce energy consumption; these rows will be dropped since they are not true outages\n",
    "rows_to_drop = ~outage_df['Alert Criteria'].str.contains('Public appeal to reduce the use of electricity')\n",
    "outage_df = outage_df[rows_to_drop]\n",
    "\n",
    "    # Rows that have no demand loss (either Null or 0) are not outages and will be dropped\n",
    "outage_df = outage_df[(outage_df['Demand Loss (MW)'] != 0) & (~outage_df['Demand Loss (MW)'].isna())].reset_index(drop = True)\n",
    "\n",
    "# 'Area Affected' Column includes information related to the state, county, and in some entries the power company responsible for that area\n",
    "# The code below will create a new column for 'State Affected' using the information from the 'Area Affected' Column\n",
    "list_of_states = ['Alabama', 'Alaska', 'American Samoa', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware', 'District of Columbia', 'Florida', 'Georgia', 'Guam', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Minor Outlying Islands', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Northern Mariana Islands', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Puerto Rico', 'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'U.S. Virgin Islands', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming']\n",
    "conditions = []\n",
    "for state in list_of_states:\n",
    "    conditions +=[(outage_df['Area Affected'].str.contains(state))]\n",
    "outage_df['State Affected'] = np.select(conditions, list_of_states, default = 'Other')\n",
    "# The code below assigns states categorized as 'Other' to their appropriate state\n",
    "outage_df[outage_df['State Affected'] == 'Other'] # To see rows where 'State Affected' is 'Other'\n",
    "outage_df.iloc[108, 8] = 'New York' # Area affected is Western NY, assigning State to New York\n",
    "outage_df.iloc[417,8] = 'North Dakota' # Area affected is Upper Greater Plains Region, assigning state to North Dakota\n",
    "outage_df.loc[outage_df['Area Affected'] == 'LUMA Energy', 'State Affected'] = 'Puerto Rico' # LUMA Energy serves Puerto Rico\n",
    "outage_df.iloc[426, 8] = 'North Dakota' # Otter Tail Power Co serves North Dakota\n",
    "outage_df = outage_df[['Datetime Event Began','Datetime of Restoration','Area Affected','State Affected','NERC Region','Alert Criteria','Event Type','Demand Loss (MW)','Number of Customers Affected']]\n",
    "outage_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03302bce-a8e5-4147-9808-61cd64a10316",
   "metadata": {},
   "source": [
    "## Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "294e7ba6-75e9-4c31-921c-b29569515e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Alabama': 'FIPS:01', 'Alaska': 'FIPS:02', 'Arizona': 'FIPS:04', 'Arkansas': 'FIPS:05', 'California': 'FIPS:06', 'Colorado': 'FIPS:08', 'Connecticut': 'FIPS:09', 'Delaware': 'FIPS:10', 'District of Columbia': 'FIPS:11', 'Florida': 'FIPS:12', 'Georgia': 'FIPS:13', 'Hawaii': 'FIPS:15', 'Idaho': 'FIPS:16', 'Illinois': 'FIPS:17', 'Indiana': 'FIPS:18', 'Iowa': 'FIPS:19', 'Kansas': 'FIPS:20', 'Kentucky': 'FIPS:21', 'Louisiana': 'FIPS:22', 'Maine': 'FIPS:23', 'Maryland': 'FIPS:24', 'Massachusetts': 'FIPS:25', 'Michigan': 'FIPS:26', 'Minnesota': 'FIPS:27', 'Mississippi': 'FIPS:28', 'Missouri': 'FIPS:29', 'Montana': 'FIPS:30', 'Nebraska': 'FIPS:31', 'Nevada': 'FIPS:32', 'New Hampshire': 'FIPS:33', 'New Jersey': 'FIPS:34', 'New Mexico': 'FIPS:35', 'New York': 'FIPS:36', 'North Carolina': 'FIPS:37', 'North Dakota': 'FIPS:38', 'Ohio': 'FIPS:39', 'Oklahoma': 'FIPS:40', 'Oregon': 'FIPS:41', 'Pennsylvania': 'FIPS:42', 'Rhode Island': 'FIPS:44', 'South Carolina': 'FIPS:45', 'South Dakota': 'FIPS:46', 'Tennessee': 'FIPS:47', 'Texas': 'FIPS:48', 'Utah': 'FIPS:49', 'Vermont': 'FIPS:50', 'Virginia': 'FIPS:51', 'Washington': 'FIPS:53', 'West Virginia': 'FIPS:54', 'Wisconsin': 'FIPS:55', 'Wyoming': 'FIPS:56'}\n"
     ]
    }
   ],
   "source": [
    "# I will use the National Centers for Environmental Information API to pull weather data at the time and location of each outage event.\n",
    "# The API overview is located here: https://www.ncdc.noaa.gov/cdo-web/webservices/v2#gettingStarted\n",
    "\n",
    "# The code below pulls the NOAA API token from a local .env file.  API token can be requested here: https://www.ncdc.noaa.gov/cdo-web/token\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "token = os.getenv('NOAA_token')\n",
    "# print(token) # This checks that the token was pulled correctly\n",
    "\n",
    "# Setting a variable \"header\" equal to the token to be included in the GET request\n",
    "headers = {\"token\": token}\n",
    "\n",
    "# Pulling data from the API regarding state location ids. Creating a dictionary of state location ids to use in subsequent GET requests.\n",
    "url_1 = \"https://www.ncei.noaa.gov/cdo-web/api/v2/locations?locationcategoryid=ST&limit=100\"\n",
    "r_1 = requests.get(url_1, headers = headers)\n",
    "json_data_1 = r_1.json()\n",
    "dict_data_1 = dict(json_data_1)\n",
    "# print(dict_data_1)\n",
    "FIPSID_dict = {dict['name']:dict['id'] for dict in dict_data_1['results']}\n",
    "# # print(FIPSID_dict)\n",
    "\n",
    "# # Pulling data from the API regarding US territory location ids. Adding these ids to the US_TERR_FIPSID dictionary.\n",
    "# url_2 = \"https://www.ncei.noaa.gov/cdo-web/api/v2/locations?locationcategoryid=US_TERR&limit=100\"\n",
    "# r_2 = requests.get(url_2, headers = headers)\n",
    "# json_data_2 = r_2.json()\n",
    "# dict_data_2 = dict(json_data_2)\n",
    "# # print(dict_data_2)\n",
    "# US_TERR_FIPSID = {dict['name']:dict['id'] for dict in dict_data_2['results']}\n",
    "# # # print(US_TERR_FIPSID)\n",
    "\n",
    "# Combining the FIPSID dictionaries\n",
    "# FIPSID_dict.update(US_TERR_FIPSID)\n",
    "print(FIPSID_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d366bdb0-b0ea-4fb7-b0e9-b2419c0323d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('keys.pickle', 'wb') as f:\n",
    "    pickle.dump(FIPSID_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a20cce-5823-432a-b41d-8b9d3fe7ab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting request parameters to pull relevant weather data from the API.\n",
    "dataset_id = \"GHCND\" # This id corresponds to the data set with historical daily climate observations. Documentation for this dataset is here: https://www.ncei.noaa.gov/pub/data/ghcn/daily/readme.txt\n",
    "\n",
    "# Below are the datatype ids of the specific observations I would like to pull from each event\n",
    "avg_temp_datatype_id = \"TAVG\" # Returns avg temperature for the date and location specified in tenths of degrees Celsius\n",
    "avg_wind_speed_datatype_id = \"AWND\" # Returns avg windspeed for the date/location specified in tenths of meters/second\n",
    "high_wind_speed_datatype_id = \"WSFI\" # Returns highest instanteneous windspeed for date/location specified in tenths of meters/second\n",
    "precip_datatype_id = \"PRCP\" # Returns precipitation for date/location in tenths of mm\n",
    "\n",
    "# Creating a function for a GET request for weather data for each event in the outage_df. \n",
    "# This code pulls the four datatypes listed above from all weather stations within the affected state, averages them over all the stations, and returns a single value for each datatype which will be incorporated in the outage_df\n",
    "def weather_data_request(event_index):\n",
    "    \n",
    "    '''\n",
    "    This function will issue a request to the NOAA CDO API for the four parameters described for a specific event in the outage_df.\n",
    "    The only argument/parameter needed is the associated index number from the outage_df dataset above.\n",
    "    '''\n",
    "    \n",
    "    start_date = outage_df['Datetime Event Began'][event_index].date()   # This will pull the start date from the outage dataset\n",
    "    end_date = start_date                                                # The end date should correspond to the start date because we are only interested in the weather at the time the outage occurred\n",
    "    location_id = FIPSID_dict[outage_df['State Affected'][event_index]]  # This pulls the affected state from the outage_df and the associated FIPSID from the FIPSID_dict\n",
    "    url_3 = f\"https://www.ncei.noaa.gov/cdo-web/api/v2/data?datasetid={dataset_id}&locationid={location_id}&startdate={start_date}&enddate={end_date}&datatypeid={avg_temp_datatype_id}&datatypeid={avg_wind_speed_datatype_id}&datatypeid={high_wind_speed_datatype_id}&datatypeid={precip_datatype_id}&limit=1000\"\n",
    "\n",
    "    max_retries = 3   # This api will often return a 503 error.  Creating a while loop to retry the request 3 times if a 503 error is returned\n",
    "    retries = 0\n",
    "    while retries <= max_retries:\n",
    "        try:\n",
    "            r_3 = requests.get(url_3, headers = headers)\n",
    "            # print(r_3)\n",
    "            if r_3.text.strip():\n",
    "                json_data_3 = r_3.json()\n",
    "                dict_data_3 = dict(json_data_3)\n",
    "                # print(dict_data_3)\n",
    "                if dict_data_3 == {}:\n",
    "                    return 'No Data'\n",
    "                else:\n",
    "                    # Checks if this data was available and then calculates avg temp over all stations in the state during date of event\n",
    "                    if [dict['value'] for dict in dict_data_3['results'] if dict['datatype']=='TAVG']!= []:\n",
    "                        avg_temp_in_C = statistics.mean([dict['value'] for dict in dict_data_3['results'] if dict['datatype']=='TAVG'])\n",
    "                        avg_temp_in_F = ((avg_temp_in_C/10)*(9/5))+32 # Converts from tenth degree Celsius to Farenheit\n",
    "                    else:\n",
    "                        avg_temp_in_F = 'No Data'\n",
    "        \n",
    "                    # Checks if this data was available and then calculates avg wind speed over all stations in state during date of event\n",
    "                    if [dict['value'] for dict in dict_data_3['results'] if dict['datatype']=='AWND']!= []:\n",
    "                        avg_windspeed_meterspersecond = statistics.mean([dict['value'] for dict in dict_data_3['results'] if dict['datatype']=='AWND'])/10 # divide by 10 to convert to meters per second\n",
    "                        avg_windspeed_mph = avg_windspeed_meterspersecond * 2.2369 # converts meters per second to miles per hour\n",
    "                    else:\n",
    "                        avg_windspeed_mph = 'No Data'\n",
    "        \n",
    "                    # Checks if this data was available and then calculates average highest wind speed in state during date of event\n",
    "                    if [dict['value'] for dict in dict_data_3['results'] if dict['datatype']=='WSFI']!= []:\n",
    "                        high_windspeed_meterspersecond = statistics.mean([dict['value'] for dict in dict_data_3['results'] if dict['datatype']=='WSFI'])/10 # divide by 10 to convert to meters per second\n",
    "                        high_windspeed_mph = high_windspeed_meterspersecond * 2.2369 # converts meters per second to miles per hour\n",
    "                    else:\n",
    "                        high_windspeed_mph = 'No Data'\n",
    "        \n",
    "                    # Checks if this data was available and then calculates average precipitation in state during date of event\n",
    "                    if [dict['value'] for dict in dict_data_3['results'] if dict['datatype']=='PRCP']!= []:\n",
    "                        avg_precip_mm = statistics.mean([dict['value'] for dict in dict_data_3['results'] if dict['datatype']=='PRCP'])/10 # divide by 10 to convert to mm\n",
    "                    else:\n",
    "                        avg_precip_mm = 'No Data'\n",
    "                    \n",
    "                    return [avg_temp_in_F, avg_windspeed_mph, high_windspeed_mph, avg_precip_mm]\n",
    "            else:\n",
    "                return 'No Data'\n",
    "        \n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if response.status_code == 503:\n",
    "                time.sleep(30)\n",
    "                retries += 1\n",
    "            else:\n",
    "                time.sleep(10)\n",
    "                retries += 1\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ded930-c994-4837-bd0f-181fe0e23f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell was  used to manually add index 407 - 411 to the weather data dictionary\n",
    "\n",
    "# start_date = outage_df['Datetime of Restoration'][411].date()   # This will pull the start date from the outage dataset\n",
    "# end_date = start_date                                                # The end date should correspond to the start date because we are only interested in the weather at the time the outage occurred\n",
    "# location_id = FIPSID_dict[outage_df['State Affected'][411]]  # This pulls the affected state from the outage_df and the associated FIPSID from the FIPSID_dict\n",
    "# url_3 = f\"https://www.ncei.noaa.gov/cdo-web/api/v2/data?datasetid={dataset_id}&locationid={location_id}&startdate={start_date}&enddate={end_date}&datatypeid={avg_temp_datatype_id}&datatypeid={avg_wind_speed_datatype_id}&datatypeid={high_wind_speed_datatype_id}&datatypeid={precip_datatype_id}&limit=1000\"\n",
    "\n",
    "# max_retries = 3\n",
    "# retries = 0\n",
    "# while retries <= max_retries:\n",
    "#     try:\n",
    "#         r_3 = requests.get(url_3, headers = headers)\n",
    "#         if r_3.text.strip():\n",
    "#             json_data_3 = r_3.json()\n",
    "#             dict_data_3 = dict(json_data_3)\n",
    "#             # print(dict_data_3)\n",
    "#             if dict_data_3 == {}:\n",
    "#                 print( 'No Data')\n",
    "#             else:\n",
    "#                 # Checks if this data was available and then calculates avg temp over all stations in the state during date of event\n",
    "#                 if [dict['value'] for dict in dict_data_3['results'] if dict['datatype']=='TAVG']!= []:\n",
    "#                     avg_temp_in_C = statistics.mean([dict['value'] for dict in dict_data_3['results'] if dict['datatype']=='TAVG'])\n",
    "#                     avg_temp_in_F = ((avg_temp_in_C/10)*(9/5))+32 # Converts from tenth degree Celsius to Farenheit\n",
    "#                 else:\n",
    "#                     avg_temp_in_F = 'No Data'\n",
    "    \n",
    "#                 # Checks if this data was available and then calculates avg wind speed over all stations in state during date of event\n",
    "#                 if [dict['value'] for dict in dict_data_3['results'] if dict['datatype']=='AWND']!= []:\n",
    "#                     avg_windspeed_meterspersecond = statistics.mean([dict['value'] for dict in dict_data_3['results'] if dict['datatype']=='AWND'])/10 # divide by 10 to convert to meters per second\n",
    "#                     avg_windspeed_mph = avg_windspeed_meterspersecond * 2.2369 # converts meters per second to miles per hour\n",
    "#                 else:\n",
    "#                     avg_windspeed_mph = 'No Data'\n",
    "    \n",
    "#                 # Checks if this data was available and then calculates average highest wind speed in state during date of event\n",
    "#                 if [dict['value'] for dict in dict_data_3['results'] if dict['datatype']=='WSFI']!= []:\n",
    "#                     high_windspeed_meterspersecond = statistics.mean([dict['value'] for dict in dict_data_3['results'] if dict['datatype']=='WSFI'])/10 # divide by 10 to convert to meters per second\n",
    "#                     high_windspeed_mph = high_windspeed_meterspersecond * 2.2369 # converts meters per second to miles per hour\n",
    "#                 else:\n",
    "#                     high_windspeed_mph = 'No Data'\n",
    "    \n",
    "#                 # Checks if this data was available and then calculates average precipitation in state during date of event\n",
    "#                 if [dict['value'] for dict in dict_data_3['results'] if dict['datatype']=='PRCP']!= []:\n",
    "#                     avg_precip_mm = statistics.mean([dict['value'] for dict in dict_data_3['results'] if dict['datatype']=='PRCP'])/10 # divide by 10 to convert to mm\n",
    "#                 else:\n",
    "#                     avg_precip_mm = 'No Data'\n",
    "                \n",
    "#                 print( [avg_temp_in_F, avg_windspeed_mph, high_windspeed_mph, avg_precip_mm])\n",
    "#                 break\n",
    "#         else:\n",
    "#             print('No Data')\n",
    "    \n",
    "#     except requests.exceptions.HTTPError as e:\n",
    "#         if response.status_code == 503:\n",
    "#             retries += 1\n",
    "#         else:\n",
    "#             retries += 1\n",
    "\n",
    "# weather_data_dict[411] = [avg_temp_in_F, avg_windspeed_mph, high_windspeed_mph, avg_precip_mm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618d98eb-c669-4bd2-a8f9-3c068d56ee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a for loop to iterate through the outage_df and pull the associated weather observations for each event. This loop will also append the weather features to the outage_df dataset\n",
    "\n",
    "# This API is very finniky and will often return a 503 error.  I created a variable to keep track of the last index updated so the for loop can continue from where it left off.\n",
    "# weather_data_dict = {}\n",
    "# last_index_updated = 426\n",
    "\n",
    "for index, row in outage_df.iterrows():\n",
    "    if last_index_updated is not None and index <= last_index_updated:\n",
    "        continue\n",
    "    else:\n",
    "        weather_data_dict[index] = weather_data_request(index)\n",
    "        last_index_updated = index\n",
    "\n",
    "# print(len(weather_data_dict))\n",
    "# last_index_updated\n",
    "\n",
    "# Note: needed to manually add data for index 407 through 411 as there is no start date, only a restoration date\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caaf5c4-47e1-464d-9f99-22934384b1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_index_updated\n",
    "weather_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26daea8e-2cd9-48c2-b4fe-a566c5b2626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from the weather dictionary and add it to the outage_df\n",
    "weather_df = pd.DataFrame.from_dict(weather_data_dict,orient = 'index',columns = ['State Avg Temp (F)', 'State Avg Windspeed (mph)', 'State High Windspeed (mph)', 'State Avg Precipitation (mm)'])\n",
    "outage_df = pd.concat([outage_df, weather_df], axis = 1)\n",
    "outage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2d98ba-dc92-4952-9793-51385bf7d0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'No Data' with Null values\n",
    "outage_df.replace('No Data', pd.NA, inplace = True)\n",
    "outage_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4448ac9c-7fa6-4bc1-91cc-0423dd17cf78",
   "metadata": {},
   "source": [
    "There are several missing values for Avg Temp, Avg Windspeed, and High Windspeed:\n",
    "- For avg temp: I may fill in with the high temperature for the day or drop them. Depending on if how EDA turns out and if the high temp is available from NOAA\n",
    "- For Avg Windspeed: I may continue with having null values in the dataset and see how EDA turns out\n",
    "- For High Windspeed: There are too many missing values.  I will drop the column.\n",
    "\n",
    "Potential for further data collection - I would like to add columns for the normal values of temperature, precipitation, and windspeed to compare to the actual observations on that day.  Further exploration of the NOAA API is needed to see if this data is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5af2bf-1ec7-4c3b-94e5-93513bc78e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the high windspeed column since there are too many missing values\n",
    "# outage_df.drop(columns=['State High Windspeed (mph)'], inplace = True)\n",
    "outage_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e37bed4-bf2f-48e4-9a80-e3ae2784287d",
   "metadata": {},
   "source": [
    "## Energy Demand Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d701a4ea-52ee-46f9-877c-beba87f96744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code below pulls the EIA API key from a local .env file.  API key can be requested here: https://www.eia.gov/opendata/documentation.php\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "API_key = os.getenv('EIA_API_key')\n",
    "# print(API_key) # This checks that the API key was pulled correctly\n",
    "\n",
    "# Electricity generation is separated into subregions. Below is a dictionary mapping the electricy generating subregions to the states they serve.  \n",
    "# This dictionary will be used to map the data pulled from the API to the outage dataframe\n",
    "elec_subba_by_state = {\n",
    "    'Minnesota': ['0001', '0027', '0035', '8910', 'WAUE'],\n",
    "    'North Dakota': ['0001', '0027', '8910', 'WAUE'],\n",
    "    'South Dakota': ['0001', '0027', 'WAUE'],\n",
    "    'Wisconsin': ['0001', '0004', '0027', '0035', '8910'],\n",
    "    'Illinois': ['0004', '0006', '0035', '8910', 'CE'],\n",
    "    'Indiana': ['0004', '0006', '8910', 'AEP'],\n",
    "    'Michigan': ['0004', '0006', '8910', 'AEP', 'ATSI'],\n",
    "    'Ohio': ['0006', '8910', 'AEP', 'AP', 'ATSI', 'DAY', 'DEOK'],\n",
    "    'Iowa': ['0027', '0035', '8910', 'WAUE'],\n",
    "    'Maine': ['4001'],\n",
    "    'New Hampshire': ['4002'],\n",
    "    'Vermont': ['4003'],\n",
    "    'Connecticut': ['4004'],\n",
    "    'Rhode Island': ['4005'],\n",
    "    'Massachusetts': ['4006', '4007', '4008'],\n",
    "    'Arkansas': ['8910', 'CSWS'],\n",
    "    'Kentucky': ['8910', 'AEP', 'DEOK', 'EKPC'],\n",
    "    'Louisiana': ['8910', 'CSWS'],\n",
    "    'Mississippi': ['8910'],\n",
    "    'Missouri': ['8910', 'CSWS', 'EDE', 'INDN', 'KACY', 'KCPL', 'MPS', 'SPRM'],\n",
    "    'New Jersey': ['AE', 'JC', 'PS', 'RECO'],\n",
    "    'Tennessee': ['AEP'],\n",
    "    'Virginia': ['AEP', 'DOM', 'DPL'],\n",
    "    'West Virginia': ['AEP', 'AP'],\n",
    "    'Maryland': ['AP', 'BC', 'DPL', 'PEP'],\n",
    "    'Pennsylvania': ['AP', 'DUQ', 'ME', 'PE', 'PL', 'PN'],\n",
    "    'Texas': ['COAS', 'CSWS', 'EAST', 'FWES', 'NCEN', 'NRTH', 'SCEN', 'SOUT', 'SPS', 'WEST'],\n",
    "    'Kansas': ['CSWS', 'EDE', 'KACY', 'KCPL', 'SECI', 'WR'],\n",
    "    'Oklahoma': ['CSWS', 'EDE', 'GRDA', 'OKGE', 'WFEC'],\n",
    "    'New Mexico': ['CYGA', 'Frep', 'Jica', 'KAFB', 'KCEC', 'LAC', 'NTUA', 'PNM', 'SPS', 'TSGT'],\n",
    "    'Delaware': ['DPL'],\n",
    "    'Nebraska': ['LES', 'NPPD', 'OPPD', 'WAUE'],\n",
    "    'Arizona': ['NTUA', 'TSGT'],\n",
    "    'Utah': ['NTUA'],\n",
    "    'California': ['PGAE', 'SCE', 'SDGE'],\n",
    "    'New York': ['RECO', 'ZONA', 'ZONB', 'ZONC', 'ZOND', 'ZONE', 'ZONF', 'ZONG', 'ZONH', 'ZONI', 'ZONJ', 'ZONK'],\n",
    "    'Colorado': ['TSGT'],\n",
    "    'Wyoming': ['TSGT'],\n",
    "    'Nevada': ['VEA'],\n",
    "    'Montana': ['WAUE'],\n",
    "    'Florida': ['FPL', 'GP', 'TECO', 'DEF', 'OUC', 'JEA', 'FPL'],\n",
    "    'Puerto Rico': ['LUMA'],\n",
    "    'Alabama': ['AP'],\n",
    "    'Alaska': ['AELP'],\n",
    "    'Georgia': ['GP'],\n",
    "    'Hawaii': ['HE'],\n",
    "    'Idaho': ['IP'],\n",
    "    'Louisiana': ['EL'],\n",
    "    'Mississippi': ['MP'],\n",
    "    'North Carolina': ['DEC', 'NCEMC'],\n",
    "    'South Carolina': ['SCEG'],\n",
    "    'Washington': ['PSE'],\n",
    "    'Oregon': ['PGE']\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d8c067-71ef-49e9-b7ea-ea342e34286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to pull energy demand data for each row in the outage dataframe\n",
    "\n",
    "def energy_demand_data_request(event_index):\n",
    "    start_date = outage_df['Datetime Event Began'][event_index].date()\n",
    "    end_date = start_date\n",
    "    \n",
    "    # The below request will pull the electrical demand data for the day of the outage event\n",
    "    url_4 = f'https://api.eia.gov/v2/electricity/rto/region-sub-ba-data/data/?api_key={API_key}&frequency=local-hourly&data[0]=value&start={start_date}T00:00:00-07:00&end={end_date}T00:00:00-07:00&offset=0&length=5000'\n",
    "    \n",
    "    try:\n",
    "        r_4 = requests.get(url_4)\n",
    "        print(r_4)\n",
    "        if r_4.text.strip():\n",
    "            json_data_4 = r_4.json()\n",
    "            dict_data_4 = dict(json_data_4)\n",
    "            # print(dict_data_4)\n",
    "            if dict_data_4 == {}:\n",
    "                return 'No Data'\n",
    "        else:\n",
    "            return 'No Data'\n",
    "            \n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        return 'error:' + e\n",
    "\n",
    "    daily_demand_mwh = sum([int(dict['value']) if dict['subba'] in elec_subba_by_state[outage_df['State Affected'].loc[event_index]] else 0 for dict in dict_data_4['response']['data']])\n",
    "    if daily_demand_mwh > 0:\n",
    "        daily_demand_mwh\n",
    "    else:\n",
    "        daily_demand_mwh = 'No Data'\n",
    "    return daily_demand_mwh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657b3beb-7db5-451a-a0ac-bb86299df828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code was used to pull index 407-411 manually since there is no beginning date for these events\n",
    "\n",
    "# manual_entry_index = 411\n",
    "# start_date = outage_df['Datetime of Restoration'][manual_entry_index].date()\n",
    "# end_date = start_date\n",
    "\n",
    "# # The below request will pull the electrical demand data for the day of the outage event\n",
    "# url_4 = f'https://api.eia.gov/v2/electricity/rto/region-sub-ba-data/data/?api_key={API_key}&frequency=local-hourly&data[0]=value&start={start_date}T00:00:00-07:00&end={end_date}T00:00:00-07:00&offset=0&length=5000'\n",
    "\n",
    "# try:\n",
    "#     r_4 = requests.get(url_4)\n",
    "#     print(r_4)\n",
    "#     if r_4.text.strip():\n",
    "#         json_data_4 = r_4.json()\n",
    "#         dict_data_4 = dict(json_data_4)\n",
    "#         print(dict_data_4)\n",
    "#         if dict_data_4 == {}:\n",
    "#             print('No Data')\n",
    "#     else:\n",
    "#         print('No Data')\n",
    "        \n",
    "# except requests.exceptions.HTTPError as e:\n",
    "#     print('error:' + e)\n",
    "\n",
    "# # The code below will use the data from the request to calculate the day's electrical demand in the affected state on the day of the outage event\n",
    "# daily_demand_mwh = sum([int(dict['value']) for dict in dict_data_4['response']['data'] if dict['subba'] in elec_subba_by_state[outage_df['State Affected'].loc[manual_entry_index]]])\n",
    "# if daily_demand_mwh > 0:\n",
    "#     print(daily_demand_mwh)\n",
    "# else:\n",
    "#     print('No Data')\n",
    "\n",
    "# energy_demand_data_dict[manual_entry_index] = daily_demand_mwh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf502593-b1b5-4592-bffa-c2af32f7ae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This for loop will iterrate through the outage_df and request the energy demand data for the specific date, calculate the daily demand for the affected state and store it in a dictionary\n",
    "\n",
    "# energy_demand_data_dict = {}\n",
    "last_index_updated = 411\n",
    "\n",
    "for index, row in outage_df.iterrows():\n",
    "    if last_index_updated is not None and index <= last_index_updated:\n",
    "        continue\n",
    "    else:\n",
    "        energy_demand_data_dict[index] = energy_demand_data_request(index)\n",
    "        last_index_updated = index\n",
    "        \n",
    "# print(energy_demand_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdef009-2527-4a71-9e92-163f2cd8091b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code below will add the energy demand data to the outage_df\n",
    "energy_demand_df = pd.DataFrame.from_dict(energy_demand_data_dict,orient = 'index',columns = ['State Daily Energy Demand (MWh)'])\n",
    "outage_df = pd.concat([outage_df, energy_demand_df], axis = 1)\n",
    "outage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d77c70-deb6-4841-a5ce-e3b72e582a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert cells with 'No Data' to null values\n",
    "outage_df.replace('No Data', pd.NA, inplace = True)\n",
    "outage_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4333ac-39b7-400e-9f62-cda822e95515",
   "metadata": {},
   "source": [
    "There are quite a few null values for daily energy demand, which may affect the analysis.  This will be explored and corrected in EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264e9087-ebec-44a8-9214-162cc75ffbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the outage_df as a csv file\n",
    "outage_df.to_csv('outage_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
